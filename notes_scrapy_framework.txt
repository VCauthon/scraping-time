CREATE SCRAPY PROJECT
    > scrapy startproject <project_name>

BASIC FILES IN SCRAPY PROJECT:
    1. scrapy.cfg > Project configuration file
    2. items.py > Saved data in scrapy project
    3. middlewares.py > Used to handle request and response
    4. pipelines.py > Used to save data in database
    5. settings.py > Used to configure project settings
    6. spiders/ > Folder contains all the scripts to scrape data
        6.1 __init__.py
        6.2 spider1.py
        6.3 spider2.py
        6.4 ...

COMMANDS
    Start spider > scrapy crawl <spider_file.py/classSpider/name>
    Create an spider > scrapy genspider <spider_name> <url>
    Save results into a file > scrapy crawl <spider_file.py/classSpider/name> -o <filename.<csv, json, xml>>

ENABLE SCRAPY SHELL > Used to test xpath and css selectors
    > scrapy shell <url>
    > response.css('css_selector').extract() > returns list of elements
    > response.xpath('xpath_selector').extract() > returns list of elements

RESPONSE METHODS
    > response.css('css_selector').extract() > returns list of elements
    > response.css('css_selector').extract_first() > returns first element
    > response.css('css_selector').extract_first(default='default_value') > returns default value if no element found
    > response.xpath('xpath_selector').extract() > returns list of elements
    > response.follow('link', callback=self.parse) > executes again the parse method with the new url

PLUGIN TO GET THE CORRECT SELECTOR css
    > https://selectorgadget.com/ (chrome)
    > inspect from firefox (has an option to copy css selector)

NOTATIONS FOR CRAWLERS
    > You can iterate over all the class where the data is stored
    > You can see what are seeing scrapy with from scrapy.utils import open_in_browser > open_in_browser(response)
    > If you want to log in with scraping do the following:
        > Open chrome and the Network tab from dev window
        > Log in and check the request that is sent
        > Copy the request as form (FORM_DATA: username, password, csrf_token)
        > Changes the spider to start with the logging in request
        > Retrieve the token tag
        > Use the FromRequest from scrapy.html to create the request
        > FromRequests.from_response(response, formdata={'csrf_token: '', username: '', password: ''}, callback=self.start_scraping)
        > Create the start_scraping method to start scraping the data (it will be like the "parse" function)
        ! SRC: https://www.youtube.com/watch?v=I_vAGDZeg5Q&list=PLhTjy8cBISEqkN-5Ku_kXG4QW33sxQo0t&index=21
    > User agent
        > You can know your own user-agent by typing in google for "what is my user agent"
        > You can set the same user-agent as google by using the google agent > googlebot user agent
        > you can use scrapy-user-agent to set the user agent in the settings.py file (https://pypi.org/project/scrapy-user-agents/)
    > Proxies
        > scrapy-proxies-pool (https://pypi.org/project/scrapy-proxies-pool/)